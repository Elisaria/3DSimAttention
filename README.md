# 3DSimAttention
This project is the official implementation of the paper titled 3DLSHAttention: Efficient 3D Point Cloud Processing via Locality-Sensitive Hashing Attention.
## About
-Core Purpose : Designing to enhance the efficiency and generalization capability of Transformers in processing 3D point clouds.  
-Use Case : 3D point cloud classification, segmentation.  
-Tech Stack: Built with TensorFlow 2.10+ (GPU support requires CUDA 11.2+) and Python 3.8+.  

